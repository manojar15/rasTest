package rastest

import org.apache.spark.sql.{SparkSession, SaveMode, Row}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.avro.Schema
import org.apache.avro.generic.{GenericRecord, GenericData}
import org.apache.avro.io.{DatumWriter, EncoderFactory}
import org.apache.avro.generic.GenericDatumWriter
import java.io.ByteArrayOutputStream
import java.nio.file.{Files, Paths}
import java.time.LocalDate
import java.time.format.DateTimeFormatter
import scala.io.Source
import scala.util.Random

object AvroEventProducerWithPartitionedFolder {
  def main(args: Array[String]): Unit = {
    // Spark setup
    val spark = SparkSession.builder()
      .appName("Partitioned Avro Event Producer (File)")
      .master("local[*]")
      .getOrCreate()
    import spark.implicits._

    // Configs/paths
    val baseOutputPath = "C:/Users/e5655076/RAS_RPT/obrandrastest/customer/avro_output"
    val customerMetaPath = "C:/Users/e5655076/RAS_RPT/obrandrastest/customer/customer_metadata"
    val logicalDate = LocalDate.parse("2025-04-03", DateTimeFormatter.ISO_DATE)
    val logicalDateDays = logicalDate.toEpochDay.toInt
    val tenantId = 42
    val eventTypes = Seq("NAME", "ADDRESS", "IDENTIFICATION")

    // Load Avro schemas
    val wrapperSchemaStr = Source.fromFile("src/main/avro/CustomerEvent.avsc").mkString
    val nameSchemaStr = Source.fromFile("src/main/avro/NamePayload.avsc").mkString
    val addressSchemaStr = Source.fromFile("src/main/avro/AddressPayload.avsc").mkString
    val idSchemaStr = Source.fromFile("src/main/avro/IdentificationPayload.avsc").mkString

    val wrapperSchema = new Schema.Parser().parse(wrapperSchemaStr)
    val headerSchema = wrapperSchema.getField("header").schema()
    val nameSchema = new Schema.Parser().parse(nameSchemaStr)
    val addressSchema = new Schema.Parser().parse(addressSchemaStr)
    val idSchema = new Schema.Parser().parse(idSchemaStr)

    // All existing customer ids
    val customerIds = spark.read.parquet(customerMetaPath)
      .select("customer_id").as[String].collect().toSeq

    val random = new Random()

    def buildEvent(
      customerId: String,
      eventType: String,
      partitionId: String,
      isNew: Boolean = false
    ): (String, Int, String, Int, String, String, Array[Byte], Array[Byte]) = {
      // Build Avro header
      val header = new GenericData.Record(headerSchema)
      header.put("event_timestamp", System.currentTimeMillis())
      header.put("logical_date", logicalDateDays)
      header.put("event_type", eventType)
      header.put("tenant_id", tenantId)
      header.put("entity_id", customerId)

      // Build Avro payload
      val (payloadSchema, payloadRecord) = eventType match {
        case "NAME" =>
          val rec = new GenericData.Record(nameSchema)
          rec.put("customer_id", customerId)
          rec.put("first", if (isNew) s"First_$customerId" else s"UpdatedFirst_$customerId")
          rec.put("middle", if (isNew) "X" else "Z")
          rec.put("last", if (isNew) s"Last_$customerId" else s"UpdatedLast_$customerId")
          (nameSchema, rec)
        case "ADDRESS" =>
          val rec = new GenericData.Record(addressSchema)
          rec.put("customer_id", customerId)
          rec.put("type", new GenericData.EnumSymbol(
            addressSchema.getField("type").schema(),
            if (random.nextBoolean()) "HOME" else "WORK"
          ))
          rec.put("street", s"AddrStreet_$customerId")
          rec.put("city", s"AddrCity_$partitionId")
          rec.put("postal_code", f"Code_${random.nextInt(1000)}%03d")
          rec.put("country", if (isNew) "CountryY" else "NewCountryX")
          (addressSchema, rec)
        case "IDENTIFICATION" =>
          val rec = new GenericData.Record(idSchema)
          rec.put("customer_id", customerId)
          rec.put("type", "passport")
          rec.put("number", s"ID_$customerId")
          rec.put("issuer", if (isNew) "GovB" else "GovX")
          (idSchema, rec)
      }

      // Avro serialize payload
      def serialize(schema: Schema, record: GenericRecord): Array[Byte] = {
        val out = new ByteArrayOutputStream()
        val encoder = EncoderFactory.get().binaryEncoder(out, null)
        val writer: DatumWriter[GenericRecord] = new GenericDatumWriter[GenericRecord](schema)
        writer.write(record, encoder)
        encoder.flush()
        out.close()
        out.toByteArray
      }
      val payloadBytes = serialize(payloadSchema, payloadRecord)

      // Build the wrapper event (header + payload)
      val wrapper = new GenericData.Record(wrapperSchema)
      wrapper.put("header", header)
      wrapper.put("payload", java.nio.ByteBuffer.wrap(payloadBytes))

      // Serialize wrapper as bytes for writing, or keep header/payload as needed
      val wrapperBytes = serialize(wrapperSchema, wrapper)

      (
        partitionId, tenantId, customerId, logicalDateDays, eventType,
        // These fields can be used for further partitioning or analysis
        // Save either the full wrapperBytes, or the header and payload as Avro schema, your call
        // We'll keep the structure for downstream access
        eventType,
        payloadBytes,
        wrapperBytes
      )
    }

    // ---- MODIFIED (existing customers): 4M events ----
    val nModified = 4000000
    val modifiedEvents = spark.sparkContext.parallelize(1 to nModified, numSlices = 64).map { _ =>
      val idx = random.nextInt(customerIds.length)
      val customerId = customerIds(idx)
      val partitionId = customerId.split("_").headOption.getOrElse("0")
      val eventType = eventTypes(random.nextInt(eventTypes.length))
      buildEvent(customerId, eventType, partitionId, isNew = false)
    }

    // ---- NEW customers: 500K X 5 events ----
    val nNewCustomers = 500000
    val newEvents = spark.sparkContext.parallelize(1 to nNewCustomers, numSlices = 32).flatMap { _ =>
      val partitionId = random.nextInt(8).toString
      val newCustomerId = f"${partitionId}_9${random.nextInt(999999)}%06d"
      // 5 events per new customer: 1 NAME, 2 ADDRESS, 2 IDENTIFICATION
      Seq(
        buildEvent(newCustomerId, "NAME", partitionId, isNew = true),
        buildEvent(newCustomerId, "ADDRESS", partitionId, isNew = true),
        buildEvent(newCustomerId, "ADDRESS", partitionId, isNew = true),
        buildEvent(newCustomerId, "IDENTIFICATION", partitionId, isNew = true),
        buildEvent(newCustomerId, "IDENTIFICATION", partitionId, isNew = true)
      )
    }

    // Convert to DataFrame
    val eventSchema = StructType(Seq(
      StructField("partition_id", StringType),
      StructField("tenant_id", IntegerType),
      StructField("customer_id", StringType),
      StructField("logical_date", IntegerType),
      StructField("event_type", StringType),
      StructField("event_type2", StringType), // same as event_type, can be removed
      StructField("payload_bytes", BinaryType),
      StructField("wrapper_bytes", BinaryType)
    ))

    val modifiedDF = spark.createDataFrame(modifiedEvents.map(Row.fromTuple), eventSchema)
    val newDF = spark.createDataFrame(newEvents.map(Row.fromTuple), eventSchema)

    // If you want to keep the Avro structure, you should NOT DROP wrapper fields.
    // You can either write the full wrapper as one Avro binary blob (hard for downstream),
    // or if your Avro schema matches, you can reconstruct the wrapper

    // For better Parquet usability, break out header fields as top-level columns before writing
    // Partition by tenant/partition/logical_date

    // ---- Write Modified ----
    modifiedDF
      .withColumn("event_timestamp", lit(System.currentTimeMillis())) // for example
      .write
      .mode(SaveMode.Overwrite)
      .partitionBy("tenant_id", "partition_id", "logical_date")
      .format("avro")
      .save(s"$baseOutputPath/modified")

    // ---- Write New ----
    newDF
      .withColumn("event_timestamp", lit(System.currentTimeMillis())) // for example
      .write
      .mode(SaveMode.Overwrite)
      .partitionBy("tenant_id", "partition_id", "logical_date")
      .format("avro")
      .save(s"$baseOutputPath/new")

    println(s"âœ… Wrote Avro events: $baseOutputPath/modified and $baseOutputPath/new, partitioned folders")

    spark.stop()
  }
}
