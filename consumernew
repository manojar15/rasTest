package rastest

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object PartitionedAvroEventConsumerBatch {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Partitioned Avro Event Consumer (Batch)")
      .master("local[*]")
      .getOrCreate()

    val avroInputBase = "C:/Users/e5655076/RAS_RPT/obrandrastest/customer/avro_output"
    val baseOutputPath = "C:/Users/e5655076/RAS_RPT/obrandrastest/customer/tenant_data"

    // Read each separately and union (avoids Spark's directory structure conflict)
    val dfModified = spark.read.format("avro").load(s"$avroInputBase/modified")
    val dfNew      = spark.read.format("avro").load(s"$avroInputBase/new")
    val avroDF     = dfModified.unionByName(dfNew)

    // Confirm the schema matches your expectations; uncomment for debugging:
    // avroDF.printSchema()

    // If your producer placed event_timestamp or payload in a different column, adjust as needed.
    // Below assumes you have these as top-level columns;
    // If not, just remove or adjust event_timestamp/payload as fits your actual schema.

    val parsed = avroDF.select(
      col("partition_id"),
      col("tenant_id"),
      col("customer_id"),
      col("event_type"),
      col("payload_bytes").as("payload"),  // or adjust to col("payload") if that's the column
      // If you want the event_timestamp, uncomment below, or remove if not present in your schema:
      // col("event_timestamp"),
      col("logical_date")
    )

    // Write as partitioned Parquet for downstream merge job
    parsed.write
      .mode("overwrite")
      .partitionBy("tenant_id", "partition_id", "logical_date")
      .format("parquet")
      .save(baseOutputPath)

    println(s"âœ… Avro events processed to Parquet partitions at $baseOutputPath")

    spark.stop()
  }
}
